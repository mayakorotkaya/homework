{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_6.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRceb2yi7Sge",
        "colab_type": "text"
      },
      "source": [
        "###Assignment 6\n",
        "[3 points] Delelop language model, which generates texts from wikipedia.\n",
        "Use WikiText-2 dataset, also available in torchtext.datasets.\n",
        "Use sentencepiece or tokenizers library for text tokenization. Pay attention to vocab size, probably subword tokens are better.\n",
        "Your model should be autogressive RNN.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QTIBzw67krv",
        "colab_type": "text"
      },
      "source": [
        "[1 point] Plot train and validation loss depending on the number of iterations of gradient decent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "By_oQHAP7p75",
        "colab_type": "text"
      },
      "source": [
        "[1 point] Try to use together (sentencepiece or tokenizers), torchtext.datasets, and torchtext.data.BPTTIterator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiAb_0G07_Oh",
        "colab_type": "text"
      },
      "source": [
        "Text generation should be terminated when either max length is reached or terminal symbol is generated.\n",
        "\n",
        "Explore several inference techniques:\n",
        "\n",
        "[1 point] Argmax\n",
        "\n",
        "[1 point] Beamsearch\n",
        "\n",
        "[1 point] Sampling from probabilty distribution with temperature\n",
        "\n",
        "[1 point] Nucleus sampling\n",
        "\n",
        "[1 point] Top-k sampling\n",
        "\n",
        "For every method you should provide implemented code and generated examples. Each example must contain at least 10 words (not subword tokens).\n",
        "Readings: https://arxiv.org/abs/1904.09751\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgzA96Jj78Z3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "9a508859-e802-4883-c0ea-13b6a47c1394"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.externals import joblib\n",
        "import nltk\n",
        "import gensim\n",
        "import spacy\n",
        "from tqdm import tqdm_notebook\n",
        "\n",
        "from sklearn import metrics\n",
        "\n",
        "import torch as tt\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchtext.data import Field, LabelField, BucketIterator, ReversibleField\n",
        "\n",
        "\n",
        "\n",
        "SEED = 42\n",
        "np.random.seed(SEED)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5zHqZXu9od6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchtext.datasets import WikiText2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uilokL0J89VP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "0a9bdb8e-1f15-4349-85d2-98e904130809"
      },
      "source": [
        "WikiText2.download('.')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading wikitext-2-v1.zip\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "wikitext-2-v1.zip: 100%|██████████| 4.48M/4.48M [00:01<00:00, 3.01MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "extracting\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./wikitext-2/wikitext-2'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnmJ4e8N91FH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "c8557cca-e82b-42b1-a877-5208cf2918b7"
      },
      "source": [
        "!pip install tokenizers"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tokenizers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3e/d8/5af89a486513ef9c1d1f7a8ad6231f3d20e2015fc84533ff7d25005c8717/tokenizers-0.2.1-cp36-cp36m-manylinux1_x86_64.whl (3.6MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 2.6MB/s \n",
            "\u001b[?25hInstalling collected packages: tokenizers\n",
            "Successfully installed tokenizers-0.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lcyCIMX-y7S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tokenizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1APiRyG-4zA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "\n",
        "spacy_en = spacy.load('en')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1xLov4EBRz7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenizer(text): # create a tokenizer function\n",
        "    return [tok.lemma_ for tok in spacy_en.tokenizer(text) if tok.text.isalpha()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBVoqjY1CAor",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "975ba2a0-6a7a-4d8d-b3aa-7a2250ae3650"
      },
      "source": [
        "tokenizer(\"Hello! It is me\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello', 'It', 'be', 'me']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlz_t5mZCJhn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d431916e-0935-47ba-8a4f-2f32aaf9d935"
      },
      "source": [
        "from spacy.lang.en import English\n",
        "nlp = English()\n",
        "sbd = nlp.create_pipe('sentencizer')\n",
        "nlp.add_pipe(sbd)\n",
        "\n",
        "text=\"Please read the analysis. (You'll be amazed.)\"\n",
        "doc = nlp(text)\n",
        "\n",
        "sents_list = []\n",
        "for sent in doc.sents:\n",
        "   sents_list.append(sent.text)\n",
        "\n",
        "print(sents_list)\n",
        "print([token.text for token in doc])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Please read the analysis. (', \"You'll be amazed.)\"]\n",
            "['Please', 'read', 'the', 'analysis', '.', '(', 'You', \"'ll\", 'be', 'amazed', '.', ')']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97dpPkDpMjJu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sentence_tokenizer(text):\n",
        "    nlp.max_length = len(text) + 1\n",
        "    return [i for i in nlp(text).sents]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsouyjQlJ9BE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "directory = './wikitext-2/wikitext-2'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyHxDNjUE86y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from os import path\n",
        "\n",
        "with open(path.join(directory, 'wiki.valid.tokens'), 'r') as f:\n",
        "    val = f.read()\n",
        "with open(path.join(directory, 'wiki.train.tokens'), 'r') as f:\n",
        "    train = f.read()\n",
        "with open(path.join(directory, 'wiki.test.tokens'), 'r') as f:\n",
        "    test = f.read()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hz2uMzQKM9i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "e67390a4-8103-4834-f43f-aecf61828fc2"
      },
      "source": [
        "print(train[:500])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " \n",
            " = Valkyria Chronicles III = \n",
            " \n",
            " Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . <unk> the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs paralle\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xYtdiyEKd6j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "raw = [val, train, test]\n",
        "prepared = []\n",
        "for text in raw:\n",
        "    sentences = sentence_tokenizer(text)\n",
        "    prepared_text = ['<start> {} <end>'.format(sent) for sent in sentences if sent]\n",
        "    prepared.append(prepared_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FLDOJ3iMxdc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "bfacfa7d-3d9b-4fd4-d621-558d801bd990"
      },
      "source": [
        "prepared[0][:5]"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<start>  \\n = Homarus gammarus = \\n \\n Homarus gammarus , known as the European lobster or common lobster , is a species of <unk> lobster from the eastern Atlantic Ocean , Mediterranean Sea and parts of the Black Sea . <end>',\n",
              " '<start> It is closely related to the American lobster , H. americanus . <end>',\n",
              " '<start> It may grow to a length of 60 cm ( 24 in ) and a mass of 6 kilograms ( 13 lb ) , and bears a conspicuous pair of claws . <end>',\n",
              " '<start> In life , the lobsters are blue , only becoming \" lobster red \" on cooking . <end>',\n",
              " '<start> Mating occurs in the summer , producing eggs which are carried by the females for up to a year before hatching into <unk> larvae . <end>']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nfYzfM6NCll",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}