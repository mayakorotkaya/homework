{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "assign_9.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4c_-LRUpEpM",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 9\n",
        "\n",
        "Use data from `https://github.com/thedenaas/hse_seminars/tree/master/2018/seminar_13/data.zip`  \n",
        "Implement model in pytorch from \"An Unsupervised Neural Attention Model for Aspect Extraction, He et al, 2017\", also desribed in seminar notes.  \n",
        "\n",
        "You can use sentence embeddings with attention **[7 points]**:  \n",
        "$z_s = \\sum_{i}^n \\alpha_i e_{w_i}, z_s \\in R^d$ sentence embedding  \n",
        "$\\alpha_i = softmax(d_i)$  attention weight for i-th token  \n",
        "$d_i = e_{w_i}^T M y_s$ attention with trainable matrix $M \\in R^{dxd}$  \n",
        "$y_s = \\frac 1 n \\sum_{i=1}^n e_{w_i}, y_s \\in R^d$ sentence context  \n",
        "$e_{w_i} \\in R^d$, token embedding of size d  \n",
        "$n$ - number of tokens in a sentence  \n",
        "\n",
        "**Or** just use sentence embedding as an average over word embeddings **[5 points]**:  \n",
        "$z_s = \\frac 1 n \\sum_{i=1}^n e_{w_i}, z_s \\in R^d$ sentence embedding  \n",
        "$e_{w_i} \\in R^d$, token embedding of size d  \n",
        "$n$ - number of tokens in a sentence  \n",
        " \n",
        "$p_t = softmax(W z_s + b), p_t \\in R^K$ topic weights for sentence $s$, with trainable matrix $W \\in R^{dxK}$ and bias vector $b \\in R^K$  \n",
        "$r_s = T^T p_t, r_s \\in R^d$ reconstructed sentence embedding as a weighted sum of topic embeddings   \n",
        "$T \\in R^{Kxd}$ trainable matrix of topic embeddings, K=number of topics\n",
        "\n",
        "\n",
        "**Training objective**:\n",
        "$$ J = \\sum_{s \\in D} \\sum_{i=1}^n max(0, 1-r_s^T z_s + r_s^T n_i) + \\lambda ||T^T T - I ||^2_F  $$\n",
        "where   \n",
        "$m$ random sentences are sampled as negative examples from dataset $D$ for each sentence $s$  \n",
        "$n_i = \\frac 1 n \\sum_{i=j}^n e_{w_j}$ average of word embeddings in the i-th sentence  \n",
        "$||T^T T - I ||_F$ regularizer, that enforces matrix $T$ to be orthogonal  \n",
        "$||A||^2_F = \\sum_{i=1}^N\\sum_{j=1}^M a_{ij}^2, A \\in R^{NxM}$ Frobenius norm\n",
        "\n",
        "\n",
        "**[3 points]** Compute topic coherence for at least for 3 different number of topics. Use 10 nearest words for each topic. It means you have to train one model for each number of topics. You can use code from seminar notes with word2vec similarity scores."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eL6PLJR1pEpW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "609887d1-b8fa-4dd0-c34a-54eb6403c50f"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.externals import joblib\n",
        "from sklearn import metrics\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "from nltk import PunktSentenceTokenizer\n",
        "nltk.download('punkt')\n",
        "from torchtext.data import Field, LabelField, BucketIterator, TabularDataset, Iterator\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.nn import init\n",
        "from torch.nn.parameter import Parameter\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHMRxhjJqpPG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c8fcfbdc-076c-4101-87bf-a3c5bd69f9f3"
      },
      "source": [
        "raw_documents = ''\n",
        "with open( \"data/data.txt\", \"r\") as f:\n",
        "    raw_documents = f.read()\n",
        "        \n",
        "print(\"Read %d raw text documents\" % len(raw_documents.split('/n')))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Read 13 raw text documents\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_v5H3c5q8is",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "be897db5-e272-4644-dfe4-aaeb45eded41"
      },
      "source": [
        "# custom stopwords\n",
        "custom_stop_words = []\n",
        "with open( \"data/stopwords.txt\", \"r\" ) as f:\n",
        "    for line in f.readlines():\n",
        "        custom_stop_words.append( line.strip().lower() )\n",
        "        \n",
        "print(\"Stopword list has %d entries\" % len(custom_stop_words) )"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Stopword list has 350 entries\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tW9ouvAtIh6s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = pd.DataFrame()\n",
        "train['doc'] = nltk.sent_tokenize(raw_documents)[:1000] "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rv9tUcV5NApG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "829213f7-5b2a-4444-a31e-0db190dcd58d"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>doc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Barclays' defiance of US fines has merit Barcl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>So it is tempting to think the bank, when aske...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>That is not the view of the chief executive, J...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Barclays thinks the DoJ’s claims are “disconne...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>But actually, some grudging respect for Staley...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 doc\n",
              "0  Barclays' defiance of US fines has merit Barcl...\n",
              "1  So it is tempting to think the bank, when aske...\n",
              "2  That is not the view of the chief executive, J...\n",
              "3  Barclays thinks the DoJ’s claims are “disconne...\n",
              "4  But actually, some grudging respect for Staley..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Jgd6cSFYOHT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train['neg1'] = train['doc'].apply(lambda x: train.iloc[np.random.choice(len(train)), 0])\n",
        "train['neg2'] = train['doc'].apply(lambda x: train.iloc[np.random.choice(len(train)), 0])\n",
        "train['neg3'] = train['doc'].apply(lambda x: train.iloc[np.random.choice(len(train)), 0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvL2-ZLPZw84",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "ca91fb58-cab3-4987-c5e0-2eb1045aa745"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>doc</th>\n",
              "      <th>neg1</th>\n",
              "      <th>neg2</th>\n",
              "      <th>neg3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Barclays' defiance of US fines has merit Barcl...</td>\n",
              "      <td>A dispatch comes in from Glastonbury, where th...</td>\n",
              "      <td>European Union referendum polling day – as it ...</td>\n",
              "      <td>But Ukip has also been getting stronger in the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>So it is tempting to think the bank, when aske...</td>\n",
              "      <td>The Real Clear Politics average uses two polls...</td>\n",
              "      <td>Spanish-owned TSB – once part of Lloyds – want...</td>\n",
              "      <td>Don’t lose this chance to make today our Indep...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>That is not the view of the chief executive, J...</td>\n",
              "      <td>The first electronic message - 1965 The very f...</td>\n",
              "      <td>I’m tired of the rich getting richer, and havi...</td>\n",
              "      <td>A Twitter spokesperson said: “Our rules explic...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Barclays thinks the DoJ’s claims are “disconne...</td>\n",
              "      <td>Ana Boata, economist at trade insurance firm E...</td>\n",
              "      <td>Whether he built the movement or simply rode i...</td>\n",
              "      <td>We’re getting some reports around the country ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>But actually, some grudging respect for Staley...</td>\n",
              "      <td>Rubio, who spent part of his childhood in Neva...</td>\n",
              "      <td>That view was echoed by Peter Waiswa, an assoc...</td>\n",
              "      <td>It’s a similar picture for Ladbrokes, which re...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 doc  ...                                               neg3\n",
              "0  Barclays' defiance of US fines has merit Barcl...  ...  But Ukip has also been getting stronger in the...\n",
              "1  So it is tempting to think the bank, when aske...  ...  Don’t lose this chance to make today our Indep...\n",
              "2  That is not the view of the chief executive, J...  ...  A Twitter spokesperson said: “Our rules explic...\n",
              "3  Barclays thinks the DoJ’s claims are “disconne...  ...  We’re getting some reports around the country ...\n",
              "4  But actually, some grudging respect for Staley...  ...  It’s a similar picture for Ladbrokes, which re...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_eNG_zFbp7A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train.to_csv('train.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDgvXL7KeFir",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize(text):\n",
        "  return nltk.word_tokenize(text) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jg1Hw1NeNDyb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEXT = Field(include_lengths=False, \n",
        "             batch_first=True, \n",
        "             tokenize=tokenize,\n",
        "             lower=True,\n",
        "             stop_words=custom_stop_words)\n",
        "\n",
        "datafields = [('doc',TEXT), ('neg1', TEXT), ('neg2', TEXT), ('neg3', TEXT)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_MLlCjprFU1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_d = TabularDataset(path=\"train.csv\", format='csv', skip_header=True, fields=datafields)\n",
        "\n",
        "TEXT.build_vocab(train_d)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4u7MkJve1Sy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f9801ff3-d64e-4383-cb34-1cee45cf5721"
      },
      "source": [
        "vocab_size = len(TEXT.vocab)\n",
        "vocab_size"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5357"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7NYuorm2d7_",
        "colab_type": "text"
      },
      "source": [
        "Thanks to https://github.com/alexeyev/abae-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGSDVi_gfAtP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SelfAttention(torch.nn.Module):\n",
        "    def __init__(self, wv_dim, maxlen):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.wv_dim = wv_dim\n",
        "\n",
        "        # max sentence length -- batch 2nd dim size\n",
        "        self.maxlen = maxlen\n",
        "        self.M = Parameter(torch.Tensor(wv_dim, wv_dim))\n",
        "        init.kaiming_uniform(self.M.data)\n",
        "\n",
        "        # softmax for attending to wod vectors\n",
        "        self.attention_softmax = torch.nn.Softmax()\n",
        "\n",
        "    def forward(self, input_embeddings):\n",
        "        # (b, wv, 1)\n",
        "        mean_embedding = torch.mean(input_embeddings, (1,)).unsqueeze(2)\n",
        "\n",
        "        # (wv, wv) x (b, wv, 1) -> (b, wv, 1)\n",
        "        product_1 = torch.matmul(self.M, mean_embedding)\n",
        "\n",
        "        # (b, maxlen, wv) x (b, wv, 1) -> (b, maxlen, 1)\n",
        "        product_2 = torch.matmul(input_embeddings, product_1).squeeze(2)\n",
        "\n",
        "        results = self.attention_softmax(product_2)\n",
        "\n",
        "        return results\n",
        "\n",
        "    def extra_repr(self):\n",
        "        return 'wv_dim={}, maxlen={}'.format(self.wv_dim, self.maxlen)\n",
        "\n",
        "\n",
        "class ABAE(torch.nn.Module):\n",
        "    \"\"\"\n",
        "        The model described in the paper ``An Unsupervised Neural Attention Model for Aspect Extraction''\n",
        "        by He, Ruidan and  Lee, Wee Sun  and  Ng, Hwee Tou  and  Dahlmeier, Daniel, ACL2017\n",
        "        https://aclweb.org/anthology/papers/P/P17/P17-1036/\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, wv_dim=200, asp_count=30, ortho_reg=0.1, maxlen=201, init_aspects_matrix=None):\n",
        "        \"\"\"\n",
        "        Initializing the model\n",
        "        :param wv_dim: word vector size\n",
        "        :param asp_count: number of aspects\n",
        "        :param ortho_reg: coefficient for tuning the ortho-regularizer's influence\n",
        "        :param maxlen: sentence max length taken into account\n",
        "        :param init_aspects_matrix: None or init. matrix for aspects\n",
        "        \"\"\"\n",
        "        super(ABAE, self).__init__()\n",
        "        self.wv_dim = wv_dim\n",
        "        self.asp_count = asp_count\n",
        "        self.ortho = ortho_reg\n",
        "        self.maxlen = maxlen\n",
        "\n",
        "        self.attention = SelfAttention(wv_dim, maxlen)\n",
        "        self.linear_transform = torch.nn.Linear(self.wv_dim, self.asp_count)\n",
        "        self.softmax_aspects = torch.nn.Softmax()\n",
        "        self.aspects_embeddings = Parameter(torch.Tensor(wv_dim, asp_count))\n",
        "\n",
        "        if init_aspects_matrix is None:\n",
        "            torch.nn.init.xavier_uniform(self.aspects_embeddings)\n",
        "        else:\n",
        "            self.aspects_embeddings.data = torch.from_numpy(init_aspects_matrix.T)\n",
        "\n",
        "    def get_aspects_importances(self, text_embeddings):\n",
        "        \"\"\"\n",
        "            Takes embeddings of a sentence as input, returns attention weights\n",
        "        \"\"\"\n",
        "\n",
        "        # compute attention scores, looking at text embeddings average\n",
        "        attention_weights = self.attention(text_embeddings)\n",
        "\n",
        "        # multiplying text embeddings by attention scores -- and summing\n",
        "        # (matmul: we sum every word embedding's coordinate with attention weights)\n",
        "        weighted_text_emb = torch.matmul(attention_weights.unsqueeze(1), # (batch, 1, sentence)\n",
        "                                         text_embeddings                 # (batch, sentence, wv_dim)\n",
        "                                         ).squeeze()\n",
        "\n",
        "        # encoding with a simple feed-forward layer (wv_dim) -> (aspects_count)\n",
        "        raw_importances = self.linear_transform(weighted_text_emb)\n",
        "\n",
        "        # computing 'aspects distribution in a sentence'\n",
        "        aspects_importances = self.softmax_aspects(raw_importances)\n",
        "\n",
        "        return attention_weights, aspects_importances, weighted_text_emb\n",
        "\n",
        "    def forward(self, text_embeddings, negative_samples_texts):\n",
        "\n",
        "        # negative samples are averaged\n",
        "        averaged_negative_samples = torch.mean(negative_samples_texts, dim=2)\n",
        "\n",
        "        # encoding: words embeddings -> sentence embedding, aspects importances\n",
        "        _, aspects_importances, weighted_text_emb = self.get_aspects_importances(text_embeddings)\n",
        "\n",
        "        # decoding: aspects embeddings matrix, aspects_importances -> recovered sentence embedding\n",
        "        recovered_emb = torch.matmul(self.aspects_embeddings, aspects_importances.unsqueeze(2)).squeeze()\n",
        "\n",
        "        # loss\n",
        "        reconstruction_triplet_loss = ABAE._reconstruction_loss(weighted_text_emb,\n",
        "                                                                recovered_emb,\n",
        "                                                                averaged_negative_samples)\n",
        "        max_margin = torch.max(reconstruction_triplet_loss, torch.zeros_like(reconstruction_triplet_loss))\n",
        "\n",
        "        return self.ortho * self._ortho_regularizer() + max_margin\n",
        "\n",
        "    @staticmethod\n",
        "    def _reconstruction_loss(text_emb, recovered_emb, averaged_negative_emb):\n",
        "\n",
        "        positive_dot_products = torch.matmul(text_emb.unsqueeze(1), recovered_emb.unsqueeze(2)).squeeze()\n",
        "        negative_dot_products = torch.matmul(averaged_negative_emb, recovered_emb.unsqueeze(2)).squeeze()\n",
        "        reconstruction_triplet_loss = torch.sum(1 - positive_dot_products.unsqueeze(1) + negative_dot_products, dim=1)\n",
        "\n",
        "        return reconstruction_triplet_loss\n",
        "\n",
        "    def _ortho_regularizer(self):\n",
        "        return torch.norm(\n",
        "            torch.matmul(self.aspects_embeddings.t(), self.aspects_embeddings) \\\n",
        "            - torch.eye(self.asp_count))\n",
        "\n",
        "    def get_aspect_words(self, w2v_model, topn=15):\n",
        "        words = []\n",
        "\n",
        "        # getting aspects embeddings\n",
        "        aspects = self.aspects_embeddings.detach().numpy()\n",
        "\n",
        "        # getting scalar products of word embeddings and aspect embeddings;\n",
        "        # to obtain the ``probabilities'', one should also apply softmax\n",
        "        words_scores = w2v_model.wv.syn0.dot(aspects)\n",
        "\n",
        "        for row in range(aspects.shape[1]):\n",
        "            argmax_scalar_products = np.argsort(- words_scores[:, row])[:topn]\n",
        "            # print([w2v_model.wv.index2word[i] for i in argmax_scalar_products])\n",
        "            # print([w for w, dist in w2v_model.similar_by_vector(aspects.T[row])[:topn]])\n",
        "            words.append([w2v_model.wv.index2word[i] for i in argmax_scalar_products])\n",
        "\n",
        "        return words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pebpb8Tiw7Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "outputId": "419d6856-834c-4ad8-e6d6-92173c48734a"
      },
      "source": [
        "import argparse\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "\n",
        "parser.add_argument(\"--word-vectors-path\", \"-wv\",\n",
        "                  dest=\"wv_path\", type=str, metavar='<str>',\n",
        "                  default=\"word_vectors/reviews_Electronics_5.json.txt.w2v\",\n",
        "                  help=\"path to word vectors file\")\n",
        "\n",
        "parser.add_argument(\"--batch-size\", \"-b\", dest=\"batch_size\", type=int, default=50,\n",
        "                  help=\"Batch size for training\")\n",
        "\n",
        "parser.add_argument(\"--aspects-number\", \"-as\", dest=\"aspects_number\", type=int, default=40,\n",
        "                  help=\"A total number of aspects\")\n",
        "\n",
        "parser.add_argument(\"--ortho-reg\", \"-orth\", dest=\"ortho_reg\", type=float, default=0.1,\n",
        "                  help=\"Ortho-regularization impact coefficient\")\n",
        "\n",
        "parser.add_argument(\"--epochs\", \"-e\", dest=\"epochs\", type=int, default=1,\n",
        "                  help=\"Epochs count\")\n",
        "\n",
        "parser.add_argument(\"--optimizer\", \"-opt\", dest=\"optimizer\", type=str, default=\"adam\", help=\"Optimizer\",\n",
        "                  choices=[\"adam\", \"adagrad\", \"sgd\"])\n",
        "\n",
        "parser.add_argument(\"--negative-samples\", \"-ns\", dest=\"neg_samples\", type=int, default=5,\n",
        "                  help=\"Negative samples per positive one\")\n",
        "\n",
        "parser.add_argument(\"--dataset-path\", \"-d\", dest=\"dataset_path\", type=str, default=\"reviews_Electronics_5.json.txt\",\n",
        "                  help=\"Path to a training texts file. One sentence per line, tokens separated wiht spaces.\")\n",
        "\n",
        "parser.add_argument(\"--maxlen\", \"-l\", type=int, default=201,\n",
        "                  help=\"Max length of the considered sentence; the rest is clipped if longer\")\n",
        "\n",
        "args = parser.parse_args()\n",
        "\n",
        "w2v_model = get_w2v(args.wv_path)\n",
        "wv_dim = w2v_model.vector_size\n",
        "y = torch.zeros(args.batch_size, 1)\n",
        "\n",
        "model = ABAE(wv_dim=wv_dim,\n",
        "            asp_count=args.aspects_number,\n",
        "            init_aspects_matrix=get_centroids(w2v_model, aspects_count=args.aspects_number))\n",
        "print(model)\n",
        "\n",
        "criterion = torch.nn.MSELoss(reduction=\"sum\")\n",
        "\n",
        "optimizer = None\n",
        "scheduler = None\n",
        "\n",
        "# if args.optimizer == \"cycsgd\":\n",
        "#     optimizer = torch.optim.SGD(model.parameters(), lr=0.05, momentum=0.9)\n",
        "#     scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=1e-5, max_lr=0.05, mode=\"triangular2\")\n",
        "# elif args.optimizer == \"adam\":\n",
        "\n",
        "if args.optimizer == \"adam\":\n",
        "  optimizer = torch.optim.Adam(model.parameters())\n",
        "elif args.optimizer == \"sgd\":\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr=0.05)\n",
        "elif args.optimizer == \"adagrad\":\n",
        "  optimizer = torch.optim.Adagrad(model.parameters())\n",
        "else:\n",
        "  raise Exception(\"Optimizer '%s' is not supported\" % args.optimizer)\n",
        "\n",
        "for t in range(args.epochs):\n",
        "\n",
        "  print(\"Epoch %d/%d\" % (t + 1, args.epochs))\n",
        "\n",
        "  data_iterator = read_data_tensors(args.dataset_path, args.wv_path,\n",
        "                                    batch_size=args.batch_size, maxlen=args.maxlen)\n",
        "\n",
        "  for item_number, (x, texts) in enumerate(data_iterator):\n",
        "      if x.shape[0] < args.batch_size:  # pad with 0 if smaller than batch size\n",
        "          x = np.pad(x, ((0, args.batch_size - x.shape[0]), (0, 0), (0, 0)))\n",
        "\n",
        "      x = torch.from_numpy(x)\n",
        "\n",
        "      # extracting bad samples from the very same batch; not sure if this is OK, so todo\n",
        "      negative_samples = torch.stack(\n",
        "          tuple([x[torch.randperm(x.shape[0])[:args.neg_samples]] for _ in range(args.batch_size)]))\n",
        "\n",
        "      # prediction\n",
        "      y_pred = model(x, negative_samples)\n",
        "\n",
        "      # error computation\n",
        "      loss = criterion(y_pred, y)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      # scheduler.step(epoch=t)\n",
        "\n",
        "      if item_number % 1000 == 0:\n",
        "\n",
        "          print(item_number, \"batches, and LR:\", optimizer.param_groups[0]['lr'])\n",
        "\n",
        "          for i, aspect in enumerate(model.get_aspect_words(w2v_model)):\n",
        "              print(i + 1, \" \".join([\"%10s\" % a for a in aspect]))\n",
        "\n",
        "          print(\"Loss:\", loss.item())\n",
        "          print()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "usage: ipykernel_launcher.py [-h] [--word-vectors-path <str>]\n",
            "                             [--batch-size BATCH_SIZE]\n",
            "                             [--aspects-number ASPECTS_NUMBER]\n",
            "                             [--ortho-reg ORTHO_REG] [--epochs EPOCHS]\n",
            "                             [--optimizer {adam,adagrad,sgd}]\n",
            "                             [--negative-samples NEG_SAMPLES]\n",
            "                             [--dataset-path DATASET_PATH] [--maxlen MAXLEN]\n",
            "ipykernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-04c4a7a4-142e-4cb0-959b-eb86a6610db9.json\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4I-TgXPXkCwV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}